{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596639414218",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Dev Split:\n",
    "\n",
    "Why do you need train-dev apart from Train, Validation & Test data\n",
    "\n",
    "### Motivation: \n",
    "after training your model on training data, if you observe\n",
    "that the performance of the model on the validation set is disappointing, you will not\n",
    "know whether this is because your model has overfit the training set, or whether this\n",
    "is just due to the data mismatch\n",
    "\n",
    "* train-dev is split from train set and not used during training the Model\n",
    "* One solution is to hold out some of the training pictures in yet\n",
    "another set that Andrew Ng calls the train-dev set. After the model is trained (on the\n",
    "training set, not on the train-dev set), you can evaluate it on the train-dev set. \n",
    "* If it performs well, then the model is not overfitting the training set. \n",
    "* If it performs poorly on the validation set, the problem must be coming from the data mismatch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE Vs MAE\n",
    "\n",
    "* RMSE is mostly preferred for regression except during outliers. In presence of outliers, MAE gives better estimate.\n",
    "* Even with one outlier, RMSE blows out the value but MAE keeps it not so bad like\n",
    "\n",
    "### Distance Norm\n",
    "* In general, $l_k$-norm:\n",
    "$$ ||v_k|| =  (|v_0|^k + |v_1|^k + .. + |v_n|^k)^{1/k} $$\n",
    "* if k = 2, gives euclidean Distance or the RMSE\n",
    "* k = 1, gives Manhattan distance or MAE\n",
    "* Higher the value of k, more it focuses or weighs on large values and neglects smaller ones. Hence, RMSE is more sensitive to outliers.\n",
    "* When outliers are rare, they perform very well and generally preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n",
    "### Points to remember:\n",
    "* Correlations are not the slope values\n",
    "* Correlations considers only linear relationships\n",
    "\n",
    "### Consider the below Example\n",
    "* The correlation coefficient only measures linear correlations (“if x\n",
    "goes up, then y generally goes up/down”). \n",
    "* It may completely miss\n",
    "out on nonlinear relationships (e.g., “if x is close to 0, then y generally goes up”). \n",
    "* Note how all the plots of the bottom row have a correlation\n",
    "coefficient equal to 0, despite the fact that their axes are\n",
    "clearly not independent: these are examples of nonlinear relationships.\n",
    "* Also, the second row shows examples where the correlation\n",
    "coefficient is equal to 1 or –1; notice that this has nothing to do\n",
    "with the slope. For example, your height in inches has a correlation\n",
    "coefficient of 1 with your height in feet or in nanometers.\n",
    "\n",
    "\n",
    "![<title>](images/end_to_end_project/Correlation.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Vs Standardization\n",
    "\n",
    "Normalization:\n",
    "* aka min-max scaling : 0-1\n",
    "\n",
    "* $\\frac {x - min}{max - min} $\n",
    "\n",
    "* through \"feature_range\" in MinMaxScaler, we can opt for range other than 0-1\n",
    "\n",
    "Standardization:\n",
    "* mean 0, std-dev 1\n",
    "\n",
    "* $ \\frac {x - mean}{\\sigma} $\n",
    "\n",
    "* Values not bound to specific ranges. This is adv in being less affected by outliers\n",
    "* Ex: If one outlier is 100, min max will crush values btw 0-15 to 0-0.15, whereas Standardization doesnt affect the other 0-15 values that much\n",
    "* But not preferred approach in neural networks which expects values btw 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why production model performance rot over time?\n",
    "\n",
    "* Even a model trained to classify pictures of cats and dogs may need to be retrained regularly, not because cats and dogs will mutate overnight, but because cameras keep changing, along with image formats, sharpness, brightness, and size ratios. Moreover, people may love different breeds next year, or they may decide to dress their pets with tiny hats—who knows?\n",
    "* Data keeps evolving and need to update dataset and re-train regularly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "### Precision & Recall\n",
    "\n",
    "* Best precision can be achieved in making one single +ve prediction and ensure it is correct.\n",
    "* But it not useful, hence it is always coupled with Recall, the ratio of +ve classes correctly identified\n",
    "\n",
    "### F1_score\n",
    "* When either of precision or recall is lower, it is not a good model.\n",
    "* F1_score being harmonic mean gives more weightage to lower values, hence F1_score will be high only when both precision & recall is high\n",
    "* If you take arithmetic mean, it gives equal weightage to precision and recall, which may not be good representative\n",
    "\n",
    "### When Precision & When Recall?\n",
    "* Precision : when FP is costly. Flagging videos for kids. You dont want to flag bad video as good and make kids see it\n",
    "* Recall : When FN is costly, Fraud detection - you dont want to leave any frauds from getting detected\n",
    "\n",
    "### Precision-Recall Trade-off\n",
    "* Varying the threshold varies the precision and recall. \n",
    "* In general, Higher threshold, lower recall and higher precision. Higher threshold may have fewer data points and hence precision will be good\n",
    "* Plot precision vs recall, initially precision will be high as threshold is very negative, but as you increase threshold precision decreases \n",
    "* Choose that threshold which gets your required precision\n",
    "* Prefer precision-recall curve over ROC especially working with imbalanced data\n",
    "\n",
    "### ROC curve\n",
    "* The roc_curve expects labels and probabilities. It keeps changing the threshold and calculates the tpr, fpr. \n",
    "* Similar to PR curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiClass Classification\n",
    "\n",
    "### One Vs All (OVA)\n",
    "* Build N number of binary classifiers one for each category. \n",
    "* Predict the class whose classifier outputs the highest score\n",
    "* 0-detector, 1-detector and so-on. Test image sent to each classifier and choose which had highest score\n",
    "\n",
    "### One Vs One (OVO)\n",
    "* Build sub-sample of one-vs-one, ie 0 vs 1, 0 vs 2, etc\n",
    "* Number of classifies : n*(n-1)/2\n",
    "* For a test images, sent it to all the classifiers & check which class wins most duels\n",
    "* Need to work on sub-sample of class of interest\n",
    "\n",
    "### Summary\n",
    "* SVM scale poorly with increase in data, so OVO is suited for svm\n",
    "* Otherwise, for most other algorithms OVR is preferred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "* Do not snoop into test data even during performing EDA\n",
    "* You might tend to create some logic which might be biased as you had seen test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "### Norm\n",
    "The norm of a vector $\\textbf{u}$, noted $\\left \\Vert \\textbf{u} \\right \\|$, is a measure of the length (a.k.a. the magnitude) of $\\textbf{u}$. There are multiple possible norms, but the most common one (and the only one we will discuss here) is the Euclidian norm, which is defined as:\n",
    "\n",
    "$\\left \\Vert \\textbf{u} \\right \\| = \\sqrt{\\sum_{i}{\\textbf{u}_i}^2}$\n",
    "\n",
    "We could implement this easily in pure python, recalling that $\\sqrt x = x^{\\frac{1}{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalized vectors\n",
    "* The **normalized vector** of a non-null vector $\\textbf{u}$, noted $\\hat{\\textbf{u}}$, is the unit vector that points in the same direction as $\\textbf{u}$. It is equal to: $\\hat{\\textbf{u}} = \\dfrac{\\textbf{u}}{\\left \\Vert \\textbf{u} \\right \\|}$\n",
    "\n",
    "### Vector Addition & Scalar Multiplication\n",
    "* vector addition results in a geometric translation\n",
    "* vector multiplication by a scalar results in rescaling (zooming in or out, centered on the origin)\n",
    "* vector dot product results in projecting a vector onto another vector, rescaling and measuring the resulting coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### Normal Equation with SVD\n",
    "* Normal Eq : \n",
    "$$ \\theta = (X^T X)^{-1} X^T y $$\n",
    "* This equation is solved by sklearn through svd approach. (lstsq & pinv formulation) \n",
    "* SVD decomposes matrix into 3 components, so its efficient in edge cases (if inverse doesnt exist - m < n or if some features are redundant)\n",
    "$$ X = U \\small \\sum V^T $$\n",
    "* In such cases, the pseudo-inverse is always defined\n",
    "* Computational Complexity \n",
    "    * Features : the Normal Eq - O($n^{2.4 - 3}$); SVD - O($n^2$)\n",
    "    * Instances : Both O(n)\n",
    "\n",
    "### Gradient Descent\n",
    "* Works well when n is large.\n",
    "* Two major disadv :\n",
    "    * Local Minima\n",
    "    * Should know when to stop\n",
    "\n",
    "### Stochastic GD:\n",
    "* Preferred if data doesnt fit the entire memory (large dataset)\n",
    "* Always shuffle the dataset (except for time-series). Bcos it can avoid if any class structure is maintained in data. Class A followed by Class B, shuffling will avoid optimizing for one class only.\n",
    "* Do not divide by n in gradient as we r working with only one data at a time\n",
    "* Learning Rate Schduler is crucial to achieve if not the optimal, closer to optimal point. Else data will keep wander (stochastic)\n",
    "* Stochastic Gradient in sklearn **doesnt require bias function** to be added!!\n",
    "* SGDClassifier in sklearn can run logisitic, SVM, models etc. not necessarily it will run specific model unless specified\n",
    "\n",
    "### Polynomial Regression\n",
    "* When there are multiple features, Polynomial Regression is capable of finding relationships between features (which is something a plain Linear Regression model cannot do). \n",
    "* This is made possible by the fact that PolynomialFeatures also\n",
    "adds all combinations of features up to the given degree. For example, if there were two features a and b, PolynomialFeatures with degree=3 would not only add the features a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.\n",
    "* It can explode the number of features added bcos of poly reg. Hence, we need to beware before performing poly reg\n",
    "\n",
    "### Regularization\n",
    "* Ridge is a good default\n",
    "* If only a few features are useful, you should prefer Lasso or Elastic Net because they tend to reduce the useless features’ weights down to zero\n",
    "* In general, Elastic Net is preferred over Lasso because Lasso\n",
    "may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.\n",
    "* Early Stopping is also a method to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Cost Function\n",
    "* It has be low at correct prediction and high for incorrect predictions for both the classes\n",
    "* cost\n",
    "    * -log(p) if y = 1\n",
    "    * -log(1 - p) if y = 0\n",
    "* log(0) is inf, log(1) = 0. Hence, with y = 1, if we predict 1 then cost goes to zero, otherwise, it goes infinity.\n",
    "* Combining both scenarios:\n",
    "$$ cost = \\frac{1}{n}*\\sum[y_i*log(\\hat y_i) + (1 - y_i)*log(1 - \\hat y_i)] $$\n",
    "* Cross-entropy loss: Generic Case\n",
    "    * $$ cost = -\\frac{1}{n} * \\sum_i^n \\sum_k^K y_k^i * log(\\hat y_k^i) $$\n",
    "    * k : number of classes, n : number of examples\n",
    "    * if wrong prediction, that sum goes to -inf, if correct it goes to zero\n",
    "    * when k = 2, this is equvilaent to above cost equation\n",
    "\n",
    "### Softmax Regression\n",
    "* Use only in-case of mutually exclusive/independent classes. Else build multiple logistic regressions\n",
    "* It is aka multinomial logisitic regression. \n",
    "* It is about multi-class prediction and not multi-output. \n",
    "* Ex : Predicting probabilities of every iris species.\n",
    "* Find $X^T * \\theta$ for each class using separate $\\theta$ vectors for each class\n",
    "* Convert it to class prob using\n",
    "    * $$ \\hat p_k = \\frac{exp (X^T * \\theta)} {\\sum^k exp (X^T * \\theta)} $$\n",
    "    * Predict the class having highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "* SVM : Large Margin classifier. There could exist only one such line maximising the split\n",
    "* Support Vectors : The instances on which the decision boundary is drawn or supported\n",
    "* SVM are sensitive to feature scales. If two features are in diff scales, the widest possible street is almost closer and horizontal. Whereas with scaling it looks much separable and wider.\n",
    "* **SVC Vs SVM** : \n",
    "    * SVC are simple support vector classifiers based on max margin classifiers (hard & soft)\n",
    "    * SVM involves : \n",
    "        1. Start with low dimension\n",
    "        2. Move data to higher dimension\n",
    "        3. Find the SVC that separates the data.\n",
    "        4. The kernel systematically finds the higher dimension required at step 2.\n",
    "\n",
    "* **Hard Margin Vs Soft Margin**:\n",
    "    * Hard Margin : Strictly impose condition. Works only for linearly separable, sensitive to outliers (impossible to fit)\n",
    "    * Soft Margin : Flexible, limit margin violations (instances in middle and wrong side)\n",
    "\n",
    "* **Calculate Margin**:\n",
    "    * Use cv to determine, allowing misclassification results in better classification in long run\n",
    "    * Use cv across all the datapoints, check for whats the bias and variance.\n",
    "    * Choose that margin, which optimizes the the bias-variance \n",
    "trade-off\n",
    "    \n",
    "* **Regularize Parameter C**:\n",
    "    * Reduce c when your model is overfitting\n",
    "    * c lower -> higher margin violations, wider street; but better generalizations\n",
    "\n",
    "* **Types of Data** :\n",
    "    * Linearly separable : Linear SVC\n",
    "    * Non-linear data : \n",
    "        * Linear SVC with polynomial features. With polynomial features, the total number of features grow and it can make model slow\n",
    "        * Kernel Trick : Through kernels, you dont generate extra features but still use the representation of polynomial featues\n",
    "\n",
    "* **Kernels** :\n",
    "    * Kernels make jobs easier as if multiple complex features are available.\n",
    "    * Gaussian, Poly, etc : Which one to choose?\n",
    "    * Start with LinearSVM (diff than SVC with linear kernel!), if n not to large -> Gaussian kernel. \n",
    "    * If compute available, then other kernels\n",
    "    * Hyperparamters:\n",
    "        * $\\gamma$ : Lower values for reducing overfitting\n",
    "        * C : Similar (low C -> high $\\lambda$ -> heavy regularization )\n",
    "\n",
    "* **Computational Complexity** :\n",
    "    * LinearSVC - O(n*m) - No kernel Trick\n",
    "    * SVC - O(n2*m) to O(n3*m) - Kernel Trick\n",
    "        * ideal for complex small-medium dataset\n",
    "    * Both requires scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression\n",
    "\n",
    "* **Concept** : \n",
    "    * Instead of maximising separating instances in classification, we need to fit all the instances within the street in Regression\n",
    "    * Width of street : $\\epsilon$ hyperparameter\n",
    "    * More the $\\epsilon$ wider the street gap. \n",
    "\n",
    "* ** Commonalities with classification**:\n",
    "    * SVC = SVR (Kernel, Kernels)\n",
    "    * LinearSVC = LinearSVR (Simple, Linear, No Kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial SVM\n",
    "\n",
    "* It is required when the data is not separable in lower dimensions (SVC fails)\n",
    "\n",
    "![title](images/svm/SVM_Polyomial_Kernel_1.PNG)\n",
    "\n",
    "* The above data if sent to squared dimensions, it becomes linearly separable.\n",
    "* Kernel Trick helps in systematically identifying the relationships in higher dimensions without tranforming them, so we can use SVC\n",
    "* Poly Kernel : \n",
    "    * $(a * b + r)^d$\n",
    "    * a, b are data points/instances;  r = poly coefficient; d = poly degree\n",
    "    * Choosing r, d is by cross-validation\n",
    "* For above dataset, if r = 1/2, d = 2 -> \n",
    "$$ (a*b + 0.5)^2$$\n",
    "$$ ab + (ab)^2 + 0.25 $$\n",
    "$$ (a, a^2, 0.5) . (b, b^2, 0.5) $$\n",
    "* The above dot product gives axis coordinates of the data in higher dimensions (squared dimensions in our case), ie pushing a -> a2 which was separable.\n",
    "* For above dataset, if r = 1, d = 2 -> we get $\\sqrt {2a}, a^2$, 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Kernel\n",
    "\n",
    "* Works for data which had overlapping classes or not separable in lower dimensions\n",
    "* Radial Kernel is like weighted nearest neigbour, gives more weightage to the neighbors in deciding\n",
    "* Radial Kernel Formula:\n",
    "    * $ exp^{\\gamma (a - b)^2} $\n",
    "    * a, b are data points; $\\gamma$ : influence scaler; High value dec the weightage.\n",
    "* Consider 3 points : a, b, c in-order of distance in data\n",
    "    * the kernel will be high btw a, b than a, classes\n",
    "    * thereby, the kernel outputs finds the relationship between points without transforming data\n",
    "* RBF is the poly kernel of r=0, d= 0 to infinity. Infinity interms of higher order dimensions! It has higher number of co-ordinates that cant be visualized. \n",
    "* RBF cooords : Roughly : (1, a, a2, a3..., inf), (1, b, b2...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}