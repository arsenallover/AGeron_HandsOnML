{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596639414218",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Dev Split:\n",
    "\n",
    "Why do you need train-dev apart from Train, Validation & Test data\n",
    "\n",
    "### Motivation: \n",
    "after training your model on training data, if you observe\n",
    "that the performance of the model on the validation set is disappointing, you will not\n",
    "know whether this is because your model has overfit the training set, or whether this\n",
    "is just due to the data mismatch\n",
    "\n",
    "* train-dev is split from train set and not used during training the Model\n",
    "* One solution is to hold out some of the training pictures in yet\n",
    "another set that Andrew Ng calls the train-dev set. After the model is trained (on the\n",
    "training set, not on the train-dev set), you can evaluate it on the train-dev set. \n",
    "* If it performs well, then the model is not overfitting the training set. \n",
    "* If it performs poorly on the validation set, the problem must be coming from the data mismatch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE Vs MAE\n",
    "\n",
    "* RMSE is mostly preferred for regression except during outliers. In presence of outliers, MAE gives better estimate.\n",
    "* Even with one outlier, RMSE blows out the value but MAE keeps it not so bad like\n",
    "\n",
    "### Distance Norm\n",
    "* In general, $l_k$-norm:\n",
    "$$ ||v_k|| =  (|v_0|^k + |v_1|^k + .. + |v_n|^k)^{1/k} $$\n",
    "* if k = 2, gives euclidean Distance or the RMSE\n",
    "* k = 1, gives Manhattan distance or MAE\n",
    "* Higher the value of k, more it focuses or weighs on large values and neglects smaller ones. Hence, RMSE is more sensitive to outliers.\n",
    "* When outliers are rare, they perform very well and generally preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n",
    "### Points to remember:\n",
    "* Correlations are not the slope values\n",
    "* Correlations considers only linear relationships\n",
    "\n",
    "### Consider the below Example\n",
    "* The correlation coefficient only measures linear correlations (“if x\n",
    "goes up, then y generally goes up/down”). \n",
    "* It may completely miss\n",
    "out on nonlinear relationships (e.g., “if x is close to 0, then y generally goes up”). \n",
    "* Note how all the plots of the bottom row have a correlation\n",
    "coefficient equal to 0, despite the fact that their axes are\n",
    "clearly not independent: these are examples of nonlinear relationships.\n",
    "* Also, the second row shows examples where the correlation\n",
    "coefficient is equal to 1 or –1; notice that this has nothing to do\n",
    "with the slope. For example, your height in inches has a correlation\n",
    "coefficient of 1 with your height in feet or in nanometers.\n",
    "\n",
    "\n",
    "![<title>](images/end_to_end_project/Correlation.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Vs Standardization\n",
    "\n",
    "Normalization:\n",
    "* aka min-max scaling : 0-1\n",
    "\n",
    "* $\\frac {x - min}{max - min} $\n",
    "\n",
    "* through \"feature_range\" in MinMaxScaler, we can opt for range other than 0-1\n",
    "\n",
    "Standardization:\n",
    "* mean 0, std-dev 1\n",
    "\n",
    "* $ \\frac {x - mean}{\\sigma} $\n",
    "\n",
    "* Values not bound to specific ranges. This is adv in being less affected by outliers\n",
    "* Ex: If one outlier is 100, min max will crush values btw 0-15 to 0-0.15, whereas Standardization doesnt affect the other 0-15 values that much\n",
    "* But not preferred approach in neural networks which expects values btw 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why production model performance rot over time?\n",
    "\n",
    "* Even a model trained to classify pictures of cats and dogs may need to be retrained regularly, not because cats and dogs will mutate overnight, but because cameras keep changing, along with image formats, sharpness, brightness, and size ratios. Moreover, people may love different breeds next year, or they may decide to dress their pets with tiny hats—who knows?\n",
    "* Data keeps evolving and need to update dataset and re-train regularly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "### Precision & Recall\n",
    "\n",
    "* Best precision can be achieved in making one single +ve prediction and ensure it is correct.\n",
    "* But it not useful, hence it is always coupled with Recall, the ratio of +ve classes correctly identified\n",
    "\n",
    "### F1_score\n",
    "* When either of precision or recall is lower, it is not a good model.\n",
    "* F1_score being harmonic mean gives more weightage to lower values, hence F1_score will be high only when both precision & recall is high\n",
    "* If you take arithmetic mean, it gives equal weightage to precision and recall, which may not be good representative\n",
    "\n",
    "### When Precision & When Recall?\n",
    "* Precision : when FP is costly. Flagging videos for kids. You dont want to flag bad video as good and make kids see it\n",
    "* Recall : When FN is costly, Fraud detection - you dont want to leave any frauds from getting detected\n",
    "\n",
    "### Precision-Recall Trade-off\n",
    "* Varying the threshold varies the precision and recall. \n",
    "* In general, Higher threshold, lower recall and higher precision. Higher threshold may have fewer data points and hence precision will be good\n",
    "* Plot precision vs recall, initially precision will be high as threshold is very negative, but as you increase threshold precision decreases \n",
    "* Choose that threshold which gets your required precision\n",
    "* Prefer precision-recall curve over ROC especially working with imbalanced data\n",
    "\n",
    "### ROC curve\n",
    "* The roc_curve expects labels and probabilities. It keeps changing the threshold and calculates the tpr, fpr. \n",
    "* Similar to PR curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiClass Classification\n",
    "\n",
    "### One Vs All (OVA)\n",
    "* Build N number of binary classifiers one for each category. \n",
    "* Predict the class whose classifier outputs the highest score\n",
    "* 0-detector, 1-detector and so-on. Test image sent to each classifier and choose which had highest score\n",
    "\n",
    "### One Vs One (OVO)\n",
    "* Build sub-sample of one-vs-one, ie 0 vs 1, 0 vs 2, etc\n",
    "* Number of classifies : n*(n-1)/2\n",
    "* For a test images, sent it to all the classifiers & check which class wins most duels\n",
    "* Need to work on sub-sample of class of interest\n",
    "\n",
    "### Summary\n",
    "* SVM scale poorly with increase in data, so OVO is suited for svm\n",
    "* Otherwise, for most other algorithms OVR is preferred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "* Do not snoop into test data even during performing EDA\n",
    "* You might tend to create some logic which might be biased as you had seen test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "### Norm\n",
    "The norm of a vector $\\textbf{u}$, noted $\\left \\Vert \\textbf{u} \\right \\|$, is a measure of the length (a.k.a. the magnitude) of $\\textbf{u}$. There are multiple possible norms, but the most common one (and the only one we will discuss here) is the Euclidian norm, which is defined as:\n",
    "\n",
    "$\\left \\Vert \\textbf{u} \\right \\| = \\sqrt{\\sum_{i}{\\textbf{u}_i}^2}$\n",
    "\n",
    "We could implement this easily in pure python, recalling that $\\sqrt x = x^{\\frac{1}{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalized vectors\n",
    "* The **normalized vector** of a non-null vector $\\textbf{u}$, noted $\\hat{\\textbf{u}}$, is the unit vector that points in the same direction as $\\textbf{u}$. It is equal to: $\\hat{\\textbf{u}} = \\dfrac{\\textbf{u}}{\\left \\Vert \\textbf{u} \\right \\|}$\n",
    "\n",
    "### Vector Addition & Scalar Multiplication\n",
    "* vector addition results in a geometric translation\n",
    "* vector multiplication by a scalar results in rescaling (zooming in or out, centered on the origin)\n",
    "* vector dot product results in projecting a vector onto another vector, rescaling and measuring the resulting coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### Normal Equation with SVD\n",
    "* Normal Eq : \n",
    "$$ \\theta = (X^T X)^{-1} X^T y $$\n",
    "* This equation is solved by sklearn through svd approach. (lstsq & pinv formulation) \n",
    "* SVD decomposes matrix into 3 components, so its efficient in edge cases (if inverse doesnt exist - m < n or if some features are redundant)\n",
    "$$ X = U \\small \\sum V^T $$\n",
    "* In such cases, the pseudo-inverse is always defined\n",
    "* Computational Complexity \n",
    "    * Features : the Normal Eq - O($n^{2.4 - 3}$); SVD - O($n^2$)\n",
    "    * Instances : Both O(n)\n",
    "\n",
    "### Gradient Descent\n",
    "* Works well when n is large.\n",
    "* Two major disadv :\n",
    "    * Local Minima\n",
    "    * Should know when to stop\n",
    "\n",
    "### Stochastic GD:\n",
    "* Preferred if data doesnt fit the entire memory (large dataset)\n",
    "* Always shuffle the dataset (except for time-series). Bcos it can avoid if any class structure is maintained in data. Class A followed by Class B, shuffling will avoid optimizing for one class only.\n",
    "* Do not divide by n in gradient as we r working with only one data at a time\n",
    "* Learning Rate Schduler is crucial to achieve if not the optimal, closer to optimal point. Else data will keep wander (stochastic)\n",
    "* Stochastic Gradient in sklearn **doesnt require bias function** to be added!!\n",
    "* SGDClassifier in sklearn can run logisitic, SVM, models etc. not necessarily it will run specific model unless specified\n",
    "\n",
    "### Polynomial Regression\n",
    "* When there are multiple features, Polynomial Regression is capable of finding relationships between features (which is something a plain Linear Regression model cannot do). \n",
    "* This is made possible by the fact that PolynomialFeatures also\n",
    "adds all combinations of features up to the given degree. For example, if there were two features a and b, PolynomialFeatures with degree=3 would not only add the features a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.\n",
    "* It can explode the number of features added bcos of poly reg. Hence, we need to beware before performing poly reg\n",
    "\n",
    "### Regularization\n",
    "* Ridge is a good default\n",
    "* If only a few features are useful, you should prefer Lasso or Elastic Net because they tend to reduce the useless features’ weights down to zero\n",
    "* In general, Elastic Net is preferred over Lasso because Lasso\n",
    "may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.\n",
    "* Early Stopping is also a method to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Cost Function\n",
    "* It has be low at correct prediction and high for incorrect predictions for both the classes\n",
    "* cost\n",
    "    * -log(p) if y = 1\n",
    "    * -log(1 - p) if y = 0\n",
    "* log(0) is inf, log(1) = 0. Hence, with y = 1, if we predict 1 then cost goes to zero, otherwise, it goes infinity.\n",
    "* Combining both scenarios:\n",
    "$$ cost = \\frac{1}{n}*\\sum[y_i*log(\\hat y_i) + (1 - y_i)*log(1 - \\hat y_i)] $$\n",
    "* Cross-entropy loss: Generic Case\n",
    "    * $$ cost = -\\frac{1}{n} * \\sum_i^n \\sum_k^K y_k^i * log(\\hat y_k^i) $$\n",
    "    * k : number of classes, n : number of examples\n",
    "    * if wrong prediction, that sum goes to -inf, if correct it goes to zero\n",
    "    * when k = 2, this is equvilaent to above cost equation\n",
    "\n",
    "### Softmax Regression\n",
    "* Use only in-case of mutually exclusive/independent classes. Else build multiple logistic regressions\n",
    "* It is aka multinomial logisitic regression. \n",
    "* It is about multi-class prediction and not multi-output. \n",
    "* Ex : Predicting probabilities of every iris species.\n",
    "* Find $X^T * \\theta$ for each class using separate $\\theta$ vectors for each class\n",
    "* Convert it to class prob using\n",
    "    * $$ \\hat p_k = \\frac{exp (X^T * \\theta)} {\\sum^k exp (X^T * \\theta)} $$\n",
    "    * Predict the class having highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "* SVM : Large Margin classifier. There could exist only one such line maximising the split\n",
    "* Support Vectors : The instances on which the decision boundary is drawn or supported\n",
    "* SVM are sensitive to feature scales. If two features are in diff scales, the widest possible street is almost closer and horizontal. Whereas with scaling it looks much separable and wider.\n",
    "* **SVC Vs SVM** : \n",
    "    * SVC are simple support vector classifiers based on max margin classifiers (hard & soft)\n",
    "    * SVM involves : \n",
    "        1. Start with low dimension\n",
    "        2. Move data to higher dimension\n",
    "        3. Find the SVC that separates the data.\n",
    "        4. The kernel systematically finds the higher dimension required at step 2.\n",
    "\n",
    "* **Hard Margin Vs Soft Margin**:\n",
    "    * Hard Margin : Strictly impose condition. Works only for linearly separable, sensitive to outliers (impossible to fit)\n",
    "    * Soft Margin : Flexible, limit margin violations (instances in middle and wrong side)\n",
    "\n",
    "* **Calculate Margin**:\n",
    "    * Use cv to determine, allowing misclassification results in better classification in long run\n",
    "    * Use cv across all the datapoints, check for whats the bias and variance.\n",
    "    * Choose that margin, which optimizes the the bias-variance \n",
    "trade-off\n",
    "    \n",
    "* **Regularize Parameter C**:\n",
    "    * Reduce c when your model is overfitting\n",
    "    * c lower -> higher margin violations, wider street; but better generalizations\n",
    "\n",
    "* **Types of Data** :\n",
    "    * Linearly separable : Linear SVC\n",
    "    * Non-linear data : \n",
    "        * Linear SVC with polynomial features. With polynomial features, the total number of features grow and it can make model slow\n",
    "        * Kernel Trick : Through kernels, you dont generate extra features but still use the representation of polynomial featues\n",
    "\n",
    "* **Kernels** :\n",
    "    * Kernels make jobs easier as if multiple complex features are available.\n",
    "    * Gaussian, Poly, etc : Which one to choose?\n",
    "    * Start with LinearSVM (diff than SVC with linear kernel!), if n not to large -> Gaussian kernel. \n",
    "    * If compute available, then other kernels\n",
    "    * Hyperparamters:\n",
    "        * $\\gamma$ : Lower values for reducing overfitting\n",
    "        * C : Similar (low C -> high $\\lambda$ -> heavy regularization )\n",
    "\n",
    "* **Computational Complexity** :\n",
    "    * LinearSVC - O(n*m) - No kernel Trick\n",
    "    * SVC - O(n2*m) to O(n3*m) - Kernel Trick\n",
    "        * ideal for complex small-medium dataset\n",
    "    * Both requires scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression\n",
    "\n",
    "* **Concept** : \n",
    "    * Instead of maximising separating instances in classification, we need to fit all the instances within the street in Regression\n",
    "    * Width of street : $\\epsilon$ hyperparameter\n",
    "    * More the $\\epsilon$ wider the street gap. \n",
    "\n",
    "* ** Commonalities with classification**:\n",
    "    * SVC = SVR (Kernel, Kernels)\n",
    "    * LinearSVC = LinearSVR (Simple, Linear, No Kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial SVM\n",
    "\n",
    "* It is required when the data is not separable in lower dimensions (SVC fails)\n",
    "\n",
    "![title](images/svm/SVM_Polyomial_Kernel_1.PNG)\n",
    "\n",
    "* The above data if sent to squared dimensions, it becomes linearly separable.\n",
    "* Kernel Trick helps in systematically identifying the relationships in higher dimensions without tranforming them, so we can use SVC\n",
    "* Poly Kernel : \n",
    "    * $(a * b + r)^d$\n",
    "    * a, b are data points/instances;  r = poly coefficient; d = poly degree\n",
    "    * Choosing r, d is by cross-validation\n",
    "* For above dataset, if r = 1/2, d = 2 -> \n",
    "$$ (a*b + 0.5)^2$$\n",
    "$$ ab + (ab)^2 + 0.25 $$\n",
    "$$ (a, a^2, 0.5) . (b, b^2, 0.5) $$\n",
    "* The above dot product gives axis coordinates of the data in higher dimensions (squared dimensions in our case), ie pushing a -> a2 which was separable.\n",
    "* For above dataset, if r = 1, d = 2 -> we get $\\sqrt {2a}, a^2$, 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Kernel\n",
    "\n",
    "* Works for data which had overlapping classes or not separable in lower dimensions\n",
    "* Radial Kernel is like weighted nearest neigbour, gives more weightage to the neighbors in deciding\n",
    "* Radial Kernel Formula:\n",
    "    * $ exp^{\\gamma (a - b)^2} $\n",
    "    * a, b are data points; $\\gamma$ : influence scaler; High value dec the weightage.\n",
    "* Consider 3 points : a, b, c in-order of distance in data\n",
    "    * the kernel will be high btw a, b than a, classes\n",
    "    * thereby, the kernel outputs finds the relationship between points without transforming data\n",
    "* RBF is the poly kernel of r=0, d= 0 to infinity. Infinity interms of higher order dimensions! It has higher number of co-ordinates that cant be visualized. \n",
    "* RBF cooords : Roughly : (1, a, a2, a3..., inf), (1, b, b2...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why computing angle useful over distance?\n",
    "\n",
    "* The answer comes in the kind of invariance we expect data to have. \n",
    "* Consider an image, and a duplicate image, where every pixel\n",
    "value is the same but 10% the brightness. The values of the individual pixels are in general far\n",
    "from the original values. Thus, if one computed the distance between the original image and the\n",
    "darker one, the distance can be large.\n",
    "* However, for most ML applications, the content is the same—it is still an image of a cat as far as a\n",
    "cat/dog classifier is concerned. \n",
    "* However, if we consider the angle, it is not hard to see that for\n",
    "any vector v, the angle between v and 0.1 · v is zero. This corresponds to the fact that scaling\n",
    "vectors keeps the same direction and just changes the length. The angle considers the darker\n",
    "image identical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "* They require minimal data preparation. Dont require feature scaling or centering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "* Curse of Dimensionality : Training slow & harder to find optimal solution. The curse of dimensionality refers to the fact that many problems that do not\n",
    "exist in low-dimensional space arise in high-dimensional space. In Machine\n",
    "Learning, one common manifestation is the fact that randomly sampled highdimensional\n",
    "vectors are generally very sparse, increasing the risk of overfitting\n",
    "and making it very difficult to identify patterns in the data without having plenty\n",
    "of training data.\n",
    "\n",
    "* Why high dimensions based predictions are unreliable:\n",
    "    \n",
    "    * Distance between two random points in high dimensions increases a lot.\n",
    "    * If you pick two points randomly in a unit\n",
    "square, the distance between these two points will be, on average, roughly 0.52. If you\n",
    "pick two random points in a unit 3D cube, the average distance will be roughly 0.66.\n",
    "But what about two points picked randomly in a 1,000,000-dimensional hypercube?\n",
    "The average distance, believe it or not, will be about 408.25 (roughly 1, 000, 000/6)!\n",
    "\n",
    "    * This is counterintuitive: how can two points be so far apart when they both lie within\n",
    "the same unit hypercube? Well, there’s just plenty of space in high dimensions. As a\n",
    "result, high-dimensional datasets are at risk of being very sparse: most training\n",
    "instances are likely to be far away from each other. \n",
    "    * This also means that a new\n",
    "instance will likely be far away from any training instance, making predictions much\n",
    "less reliable than in lower dimensions, since they will be based on much larger extrapolations.\n",
    "In short, the more dimensions the training set has, the greater the risk of\n",
    "overfitting it.\n",
    "\n",
    "* Increase the training size to reach sufficient density of training instances to avoid curse of Dimensionality.\n",
    "\n",
    "* Unfortunately that level of density is not achievable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "* Projection:\n",
    "    * data is not uniform across all dimensions. ie features may be constant, correlated -> can be made to lie in lower dimensions\n",
    "    * But projection doesnt aways guarentee as best approach to Dimensionality reduction.\n",
    "    * Conider swiss roll example projection :\n",
    "        * Swiss roll needs to unrolled and not projected to get better representation.\n",
    "        * We want lower dimension representation of right (unfolding) and not left (projection)\n",
    "\n",
    "![title](images\\dimensionalty_reduction\\Proj_1.PNG)\n",
    "    \n",
    "* Manifold :\n",
    "\n",
    "    * Manifold assumption : Many high dimensional datasets lie close to much lower dimensional manifold\n",
    "    *  Swiss unroll is example of 2D manifold (is a 2D shape that can be bent & twisted in a higher dimensional space)\n",
    "    * Swiss roll locally resembles 2d place but it is rolled in 3rd dimension\n",
    "    * Manifold learning may be separable at higher dimensions too.  \n",
    "![title](images\\dimensionalty_reduction\\Proj_2.PNG)\n",
    "\n",
    "* Despite, dimensionalty_reduction helps in speeding training -> may not always lead to better results. It depends on dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "\n",
    "* Perceptrons are mostly single layer neural networks\n",
    "\n",
    "* It takes inputs -> weighted sum of inputs -> applies step function to that sum -> outputs results\n",
    "\n",
    "* Output, h(x) = step(z), where z = $w^T w$ \n",
    "\n",
    "* Most common step function : Considering threshold at 0\n",
    "\n",
    "\\begin{equation}\n",
    "  heaviside \\ (z) =\n",
    "    \\begin{cases}\n",
    "      0 \\ \\text{is z < 0}\\\\\n",
    "      1 \\ \\text{is z >= 0} \\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "\n",
    "* Here the step function is the activation function\n",
    "\n",
    "* Train Perceptrons:\n",
    "  * Use Hebbs rule - strengthen connections that help reduce error\n",
    "\n",
    "  * ie go one data at a time -> strengthen weights from inputs that lead to correct prediction\n",
    "  \n",
    "  * $ w_{nextstep} = w + \\eta (y - \\hat y)x $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Random Weights Init\n",
    "\n",
    "It is important to initialize all the hidden layers’ connection weights\n",
    "randomly, or else training will fail. For example, if you initialize all\n",
    "weights and biases to zero, then all neurons in a given layer will be\n",
    "perfectly identical, and thus backpropagation will affect them in\n",
    "exactly the same way, so they will remain identical. In other words,\n",
    "despite having hundreds of neurons per layer, your model will act\n",
    "as if it had only one neuron per layer: it won’t be too smart. If\n",
    "instead you randomly initialize the weights, you break the symmetry\n",
    "and allow backpropagation to train a diverse team of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Activation Functions?\n",
    "\n",
    "In Perceptron, Replace step function with the logistic (sigmoid) function,\n",
    "σ(z) = 1 / (1 + exp(–z)). This was essential because the step function contains\n",
    "only flat segments, so there is no gradient to work with (Gradient Descent cannot\n",
    "move on a flat surface), while the logistic function has a well-defined nonzero derivative\n",
    "everywhere, allowing Gradient Descent to make some progress at every step\n",
    "\n",
    "Well, if\n",
    "you chain several linear transformations, all you get is a linear transformation. For\n",
    "example, if f(x) = 2x + 3 and g(x) = 5x – 1, then chaining these two linear functions\n",
    "gives you another linear function: f(g(x)) = 2(5x – 1) + 3 = 10x + 1. So if you don’t\n",
    "have some nonlinearity between layers, then even a deep stack of layers is equivalent\n",
    "to a single layer, and you can’t solve very complex problems with that. Conversely, a\n",
    "large enough DNN with nonlinear activations can theoretically approximate any continuous\n",
    "function.\n"
   ]
  },
  {
   "source": [
    "* RelU & Variants:\n",
    "    * RelU : max(0, z)\n",
    "    * Relu is faster than sigmoid due to comparitively complex gradient calculations \n",
    "    * Grads die at negative areas (ie when the sum of weighted inputs are -) -> no sgd update \n",
    "\n",
    "* Softplus : log(1 + exp(z))\n",
    "    * More smoother version of RelU (in terms of differentiation)\n",
    "    * close to 0 when -\n",
    "    * close to z when +\n",
    "    * Smoother than RelU\n",
    "\n",
    "* Leaky RelU:\n",
    "    * max($\\alpha$ z, z)\n",
    "    * $\\alpha$ -> determines how much leak it can take ~ 0.01.\n",
    "    * Ensures they dont go to coma/die out\n",
    "\n",
    "* RReLU:\n",
    "    * Randomized Leaky RelU\n",
    "    * $\\alpha$ is picked randomly from given range & fixed to avg during testing\n",
    "\n",
    "* PReLU:\n",
    "    * Parametric ReLU \n",
    "    * $\\alpha$ is learned during training\n",
    "    * Better with large datasets, poor with small datasets (overfitting)\n",
    "\n",
    "* ELU:\n",
    "    * Exponential Linear Unit. Exp decreasing when -\n",
    "    * $\\alpha$(exp (z) - 1) if -\n",
    "    * z if +\n",
    "    * Slower compute\n",
    "\n",
    "* SELU:\n",
    "    * Scaled ELU\n",
    "    * Self -Normalize : if all hidden layers had SELu -> each output layers will have mean 0, stddev 1 -> no vanishing/exloding grad problem\n",
    "    * Constraints:\n",
    "        * Inputs to be normalized\n",
    "        * Weights -> lecun normalization\n",
    "        * WOrks only with sequential data\n",
    "\n",
    "* Summary :\n",
    "    * SELU > ELU > leak RELU > RELU > Tanh > sigmoid\n",
    "    * If self-normalization not possible ELu > SELU\n",
    "    * Run time latency -> LRELU  > SELU\n",
    "\n",
    "\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "![title](images\\ann\\loss.PNG)\n",
    "\n",
    "* MSE:\n",
    "    * Standard but sensitive to outliers.\n",
    "    * Dont use if u have lot of outliers, weights can tend to be biased towards fitting those outliers\n",
    "    * Gradient descent is efficient than MAE, as towards optimum the slope is low -> wont oscillate near optimum -> faster convergence\n",
    "\n",
    "* MAE:\n",
    "    * RObust to outliers\n",
    "    * Differentiable, but constant slope leads to wandering of Gradient descent updates while trying to reach optimum.\n",
    "\n",
    "* Huber Loss:\n",
    "    * Mix of goods from MAE and MSE.\n",
    "    * It is quadratic (like MSE) when error is low and linear(like MAE) when error is high (outliers)\n",
    "    * Has extra hyperparameter which accounts for how much low than error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images\\ann\\regression_1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## losses:\n",
    "\n",
    "* Categorical Cross Entropy:\n",
    "    * Multi class classification\n",
    "    * Use when labels are one hot encoded ie [0, 0, 1] for class 3\n",
    "    * Softmax activation\n",
    "    \n",
    "* Sparse Categorical Cross Entropy:\n",
    "    * Multi class classification\n",
    "    * When labes are just class index like 0, 2, 4 etc. \n",
    "    * Efficient that u need not store big one hot encoded big matrix.\n",
    "    * Softmax activation\n",
    "\n",
    "* Binary Cross Entropy:\n",
    "    * Binary classification\n",
    "    * Sigmoid activation follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Weights Vs Class Weights\n",
    "\n",
    "* If the training set was very skewed, with some classes being overrepresented and others\n",
    "underrepresented, it would be useful to set the class_weight argument when\n",
    "calling the fit() method, which would give a larger weight to underrepresented\n",
    "classes and a lower weight to overrepresented classes. These weights would be used by\n",
    "Keras when computing the loss.\n",
    "\n",
    "* If you need per-instance weights, set the sam\n",
    "ple_weight argument (if both class_weight and sample_weight are provided, Keras\n",
    "multiplies them). Per-instance weights could be useful if some instances were labeled\n",
    "by experts while others were labeled using a crowdsourcing platform: you might want\n",
    "to give more weight to the former. You can also provide sample weights (but not class\n",
    "weights) for the validation set by adding them as a third item in the validation_data\n",
    "tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional, Sequential & Subclassing API\n",
    "\n",
    "* Sequential API is very common, written in sequence\n",
    "    * Cannot add complex structure - like concat layers etc\n",
    "\n",
    "    * model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape = [28, 28]),\n",
    "        keras.layers.Dense(300, activation = 'relu'),\n",
    "        keras.layers.Dense(100, activation = 'relu'),\n",
    "        keras.layers.Dense(10, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "* Functional API:\n",
    "    * Can build complex models - wide and deep NN\n",
    "    * Connects all or parts of inputs directly to output layers\n",
    "    * It can learn deep patterns (longest path) & simple rules (short path) simultaneously\n",
    "    * Can provide different subset of inputs to each branch in the NN\n",
    "    * It is called functional bcos it is written as function with input arguments           \n",
    "    \n",
    "    hidden1 = keras.layers.Dense(30, activation = 'relu')(input_)       \n",
    "    hidden2 = keras.layers.Dense(30, activation = 'relu')(hidden1)          \n",
    "    concat = keras.layers.Concatenate()([input_, hidden1])          \n",
    "    output = keras.layers.Dense(10, activation = 'softmax')(concat)     \n",
    "\n",
    "model = keras.Model(inputs = [input_], outputs = [output])\n",
    "    \n",
    "* Adv of above two:\n",
    "    * Easy save, clone, shared\n",
    "    * Structure can be displayed, analysed (can infer shapes)\n",
    "    * Errors can be caught easily, easy debug\n",
    "\n",
    "* Subclassing:\n",
    "    * Main disadv of above two : static - whole model is static graph of layers\n",
    "    * Diff scenarios - we need diff shapes, conditional branching, looping\n",
    "    * Those can be achieved by Subclassing "
   ]
  },
  {
   "source": [
    "### Model Summary \n",
    "\n",
    "input_ = keras.layers.Input(shape = xtrain.shape[1])    \n",
    "hidden1 = keras.layers.Dense(24, activation = 'relu')(input_)   \n",
    "hidden2 = keras.layers.Dense(50, activation = 'relu')(hidden1)  \n",
    "output = keras.layers.Dense(1)(hidden2)     \n",
    "\n",
    "model = keras.models.Model(inputs = [input_], outputs = [output])\n",
    "\n",
    "#### Assuming Batch_size of 32 and xtrain of features 8\n",
    "\n",
    "![title](images\\ann\\NN_Summary.PNG)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "* There are lot of evolutionary algorithms which perform automatic Tuning\n",
    "\n",
    "* In general you will get more bang for your buck by increasing the\n",
    "number of layers instead of the number of neurons per layer.\n",
    "\n",
    "* Try to build bigger layers & neurons than expected and then do early stopping and regularization\n",
    "\n",
    "* One way to find a good learning rate is to train the model for a few hundred iterations,\n",
    "starting with a very low learning rate (e.g., 10-5) and gradually increasing\n",
    "it up to a very large value (e.g., 10). This is done by multiplying the learning rate\n",
    "by a constant factor at each iteration (e.g., by exp(log(106)/500) to go from 10-5 to\n",
    "10 in 500 iterations).\n",
    "\n",
    "* one strategy is to try to use a large batch size, using learning rate warmup, and if training is unstable or the final performance is disappointing, then try using a small batch size instead."
   ]
  }
 ]
}