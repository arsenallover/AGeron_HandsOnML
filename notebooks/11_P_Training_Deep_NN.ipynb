{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600862450807",
   "display_name": "Python 3.7.9 64-bit ('tf2.0': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(xtrain, ytrain), (xtest, ytest) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest = xtrain/255.0, xtest/255.0\n",
    "xval, yval = xtrain[50000:], ytrain[50000:]\n",
    "xtrain, ytrain = xtrain[:50000], ytrain[:50000]"
   ]
  },
  {
   "source": [
    "## 1. Testing different activations\n",
    "### 1.1 Relu"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 50000 samples, validate on 10000 samples\nEpoch 1/10\n50000/50000 [==============================] - 3s 66us/sample - loss: 1.2976 - accuracy: 0.6326 - val_loss: 0.8870 - val_accuracy: 0.7233\nEpoch 2/10\n50000/50000 [==============================] - 2s 45us/sample - loss: 0.7866 - accuracy: 0.7445 - val_loss: 0.7171 - val_accuracy: 0.7582\nEpoch 3/10\n50000/50000 [==============================] - 3s 60us/sample - loss: 0.6714 - accuracy: 0.7785 - val_loss: 0.6450 - val_accuracy: 0.7861\nEpoch 4/10\n50000/50000 [==============================] - 2s 48us/sample - loss: 0.6111 - accuracy: 0.7988 - val_loss: 0.5982 - val_accuracy: 0.7996\nEpoch 5/10\n50000/50000 [==============================] - 3s 58us/sample - loss: 0.5720 - accuracy: 0.8114 - val_loss: 0.5954 - val_accuracy: 0.7873\nEpoch 6/10\n50000/50000 [==============================] - 2s 50us/sample - loss: 0.5442 - accuracy: 0.8194 - val_loss: 0.5419 - val_accuracy: 0.8182\nEpoch 7/10\n50000/50000 [==============================] - 2s 47us/sample - loss: 0.5229 - accuracy: 0.8255 - val_loss: 0.5217 - val_accuracy: 0.8225\nEpoch 8/10\n50000/50000 [==============================] - 3s 60us/sample - loss: 0.5064 - accuracy: 0.8299 - val_loss: 0.5091 - val_accuracy: 0.8241\nEpoch 9/10\n50000/50000 [==============================] - 3s 62us/sample - loss: 0.4931 - accuracy: 0.8335 - val_loss: 0.5385 - val_accuracy: 0.8018\nEpoch 10/10\n50000/50000 [==============================] - 3s 62us/sample - loss: 0.4815 - accuracy: 0.8355 - val_loss: 0.5055 - val_accuracy: 0.8187\n"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    tf.keras.layers.Dense(300, activation='relu',kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='relu',kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'], optimizer = 'SGD')\n",
    "history = model.fit(xtrain, ytrain, validation_data = (xval, yval), epochs = 10, batch_size = 256)\n",
    "relu_acc, relu_valacc = history.history['accuracy'], history.history['val_accuracy']\n",
    "relu_loss, relu_valloss = history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "source": [
    "### 1.2 LeakyRELU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 50000 samples, validate on 10000 samples\nEpoch 1/10\n50000/50000 [==============================] - 4s 74us/sample - loss: 1.2348 - accuracy: 0.6330 - val_loss: 0.8538 - val_accuracy: 0.7213\nEpoch 2/10\n50000/50000 [==============================] - 3s 62us/sample - loss: 0.7643 - accuracy: 0.7459 - val_loss: 0.6991 - val_accuracy: 0.7625\nEpoch 3/10\n50000/50000 [==============================] - 3s 58us/sample - loss: 0.6572 - accuracy: 0.7812 - val_loss: 0.6315 - val_accuracy: 0.7883\nEpoch 4/10\n50000/50000 [==============================] - 3s 54us/sample - loss: 0.6005 - accuracy: 0.8024 - val_loss: 0.5881 - val_accuracy: 0.7994\nEpoch 5/10\n50000/50000 [==============================] - 2s 45us/sample - loss: 0.5641 - accuracy: 0.8139 - val_loss: 0.5827 - val_accuracy: 0.7931\nEpoch 6/10\n50000/50000 [==============================] - 4s 80us/sample - loss: 0.5381 - accuracy: 0.8204 - val_loss: 0.5354 - val_accuracy: 0.8179\nEpoch 7/10\n50000/50000 [==============================] - 3s 54us/sample - loss: 0.5183 - accuracy: 0.8269 - val_loss: 0.5164 - val_accuracy: 0.8248\nEpoch 8/10\n50000/50000 [==============================] - 3s 52us/sample - loss: 0.5029 - accuracy: 0.8309 - val_loss: 0.5044 - val_accuracy: 0.8263\nEpoch 9/10\n50000/50000 [==============================] - 4s 70us/sample - loss: 0.4907 - accuracy: 0.8340 - val_loss: 0.5325 - val_accuracy: 0.8040\nEpoch 10/10\n50000/50000 [==============================] - 4s 72us/sample - loss: 0.4800 - accuracy: 0.8360 - val_loss: 0.5036 - val_accuracy: 0.8199\n"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    tf.keras.layers.Dense(300, kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'], optimizer = 'SGD')\n",
    "history = model.fit(xtrain, ytrain, validation_data = (xval, yval), epochs = 10, batch_size = 256)\n",
    "leaky_relu_acc, leaky_relu_valacc = history.history['accuracy'], history.history['val_accuracy']\n",
    "leaky_relu_loss, leaky_relu_valloss = history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "source": [
    "### 1.3 Parametric ReLU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 50000 samples, validate on 10000 samples\nEpoch 1/10\n50000/50000 [==============================] - 4s 79us/sample - loss: 1.2968 - accuracy: 0.6328 - val_loss: 0.8866 - val_accuracy: 0.7230\nEpoch 2/10\n50000/50000 [==============================] - 3s 55us/sample - loss: 0.7863 - accuracy: 0.7446 - val_loss: 0.7169 - val_accuracy: 0.7579\nEpoch 3/10\n50000/50000 [==============================] - 3s 60us/sample - loss: 0.6712 - accuracy: 0.7786 - val_loss: 0.6449 - val_accuracy: 0.7859\nEpoch 4/10\n50000/50000 [==============================] - 3s 55us/sample - loss: 0.6109 - accuracy: 0.7988 - val_loss: 0.5981 - val_accuracy: 0.7990\nEpoch 5/10\n50000/50000 [==============================] - 3s 64us/sample - loss: 0.5719 - accuracy: 0.8112 - val_loss: 0.5954 - val_accuracy: 0.7874\nEpoch 6/10\n50000/50000 [==============================] - 3s 56us/sample - loss: 0.5440 - accuracy: 0.8195 - val_loss: 0.5417 - val_accuracy: 0.8181\nEpoch 7/10\n50000/50000 [==============================] - 3s 67us/sample - loss: 0.5228 - accuracy: 0.8254 - val_loss: 0.5215 - val_accuracy: 0.8224\nEpoch 8/10\n50000/50000 [==============================] - 3s 56us/sample - loss: 0.5063 - accuracy: 0.8300 - val_loss: 0.5090 - val_accuracy: 0.8243\nEpoch 9/10\n50000/50000 [==============================] - 3s 68us/sample - loss: 0.4931 - accuracy: 0.8334 - val_loss: 0.5382 - val_accuracy: 0.8018\nEpoch 10/10\n50000/50000 [==============================] - 3s 59us/sample - loss: 0.4815 - accuracy: 0.8356 - val_loss: 0.5055 - val_accuracy: 0.8189\n"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    tf.keras.layers.Dense(300, kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.PReLU(),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.PReLU(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'], optimizer = 'SGD')\n",
    "history = model.fit(xtrain, ytrain, validation_data = (xval, yval), epochs = 10, batch_size = 256)\n",
    "prelu_acc, prelu_valacc = history.history['accuracy'], history.history['val_accuracy']\n",
    "prelu_loss, prelu_valloss = history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "source": [
    "### 1.4 ELU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 50000 samples, validate on 10000 samples\nEpoch 1/10\n50000/50000 [==============================] - 4s 72us/sample - loss: 1.0814 - accuracy: 0.6569 - val_loss: 0.7694 - val_accuracy: 0.7409\nEpoch 2/10\n50000/50000 [==============================] - 3s 57us/sample - loss: 0.6977 - accuracy: 0.7662 - val_loss: 0.6477 - val_accuracy: 0.7762\nEpoch 3/10\n50000/50000 [==============================] - 3s 54us/sample - loss: 0.6111 - accuracy: 0.7962 - val_loss: 0.5927 - val_accuracy: 0.7961\nEpoch 4/10\n50000/50000 [==============================] - 3s 53us/sample - loss: 0.5646 - accuracy: 0.8117 - val_loss: 0.5566 - val_accuracy: 0.8050\nEpoch 5/10\n50000/50000 [==============================] - 3s 60us/sample - loss: 0.5349 - accuracy: 0.8200 - val_loss: 0.5499 - val_accuracy: 0.8013\nEpoch 6/10\n50000/50000 [==============================] - 3s 51us/sample - loss: 0.5139 - accuracy: 0.8259 - val_loss: 0.5144 - val_accuracy: 0.8198\nEpoch 7/10\n50000/50000 [==============================] - 3s 58us/sample - loss: 0.4979 - accuracy: 0.8302 - val_loss: 0.4995 - val_accuracy: 0.8240\nEpoch 8/10\n50000/50000 [==============================] - 3s 51us/sample - loss: 0.4852 - accuracy: 0.8341 - val_loss: 0.4890 - val_accuracy: 0.8255\nEpoch 9/10\n50000/50000 [==============================] - 3s 56us/sample - loss: 0.4751 - accuracy: 0.8373 - val_loss: 0.5026 - val_accuracy: 0.8161\nEpoch 10/10\n50000/50000 [==============================] - 3s 50us/sample - loss: 0.4661 - accuracy: 0.8390 - val_loss: 0.4840 - val_accuracy: 0.8249\n"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    tf.keras.layers.Dense(300, activation = 'elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation = 'elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'], optimizer = 'SGD')\n",
    "history = model.fit(xtrain, ytrain, validation_data = (xval, yval), epochs = 10, batch_size = 256)\n",
    "elu_acc, elu_valacc = history.history['accuracy'], history.history['val_accuracy']\n",
    "elu_loss, elu_valloss = history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "source": [
    "### 1.5 SELU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 50000 samples, validate on 10000 samples\nEpoch 1/10\n50000/50000 [==============================] - 4s 80us/sample - loss: 0.7438 - accuracy: 0.7509 - val_loss: 0.5591 - val_accuracy: 0.8074\nEpoch 2/10\n50000/50000 [==============================] - 3s 54us/sample - loss: 0.5066 - accuracy: 0.8252 - val_loss: 0.4985 - val_accuracy: 0.8274\nEpoch 3/10\n50000/50000 [==============================] - 3s 62us/sample - loss: 0.4564 - accuracy: 0.8421 - val_loss: 0.4688 - val_accuracy: 0.8362\nEpoch 4/10\n50000/50000 [==============================] - 3s 53us/sample - loss: 0.4276 - accuracy: 0.8521 - val_loss: 0.4460 - val_accuracy: 0.8439\nEpoch 5/10\n50000/50000 [==============================] - 3s 64us/sample - loss: 0.4082 - accuracy: 0.8577 - val_loss: 0.4395 - val_accuracy: 0.8461\nEpoch 6/10\n50000/50000 [==============================] - 3s 55us/sample - loss: 0.3935 - accuracy: 0.8628 - val_loss: 0.4253 - val_accuracy: 0.8496\nEpoch 7/10\n50000/50000 [==============================] - 3s 65us/sample - loss: 0.3810 - accuracy: 0.8678 - val_loss: 0.4148 - val_accuracy: 0.8564\nEpoch 8/10\n50000/50000 [==============================] - 3s 54us/sample - loss: 0.3708 - accuracy: 0.8702 - val_loss: 0.4078 - val_accuracy: 0.8578\nEpoch 9/10\n50000/50000 [==============================] - 3s 56us/sample - loss: 0.3619 - accuracy: 0.8739 - val_loss: 0.4038 - val_accuracy: 0.8581\nEpoch 10/10\n50000/50000 [==============================] - 3s 59us/sample - loss: 0.3536 - accuracy: 0.8772 - val_loss: 0.3991 - val_accuracy: 0.8612\n"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "pixel_means = xtrain.mean(axis=0, keepdims=True)\n",
    "pixel_stds = xtrain.std(axis=0, keepdims=True)\n",
    "xtrain_scaled = (xtrain - pixel_means) / pixel_stds\n",
    "xval_scaled = (xval - pixel_means) / pixel_stds\n",
    "xtest_scaled = (xtest - pixel_means) / pixel_stds\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    tf.keras.layers.Dense(300, activation = 'selu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation = 'selu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'], optimizer = 'SGD')\n",
    "history = model.fit(xtrain_scaled, ytrain, validation_data = (xval_scaled, yval), epochs = 10, batch_size = 256)\n",
    "selu_acc, selu_valacc = history.history['accuracy'], history.history['val_accuracy']\n",
    "selu_loss, selu_valloss = history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-29c3af6c81ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m121\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Accuracy\")\n",
    "sns.lineplot(np.arange(10), relu_acc, label='RELU')\n",
    "sns.lineplot(np.arange(10), leaky_relu_acc, label='Leaky_RELU')\n",
    "sns.lineplot(np.arange(10), prelu_acc, label='PRELU')\n",
    "sns.lineplot(np.arange(10), elu_acc, label='ELU')\n",
    "sns.lineplot(np.arange(10), selu_acc, label='SELU')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Validation Accuracy\")\n",
    "sns.lineplot(np.arange(10), relu_valacc, label='RELU')\n",
    "sns.lineplot(np.arange(10), leaky_relu_valacc, label='Leaky_RELU')\n",
    "sns.lineplot(np.arange(10), prelu_valacc, label='PRELU')\n",
    "sns.lineplot(np.arange(10), elu_valacc, label='ELU')\n",
    "sns.lineplot(np.arange(10), selu_valacc, label='SELU')\n"
   ]
  },
  {
   "source": [
    "## Observations\n",
    "\n",
    "* Despite running for short epochs, SELU clearly outperforming other variants\n",
    "* As the usual trend goes, we see SELU > ELU > PRELU, LeakyRelu > RELU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Batch Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 50000 samples, validate on 10000 samples\nEpoch 1/10\n50000/50000 [==============================] - 6s 129us/sample - loss: 0.8322 - accuracy: 0.7251 - val_loss: 0.8040 - val_accuracy: 0.7709\nEpoch 2/10\n50000/50000 [==============================] - 5s 98us/sample - loss: 0.5372 - accuracy: 0.8168 - val_loss: 0.5270 - val_accuracy: 0.8184\nEpoch 3/10\n50000/50000 [==============================] - 5s 101us/sample - loss: 0.4763 - accuracy: 0.8331 - val_loss: 0.4731 - val_accuracy: 0.8306\nEpoch 4/10\n50000/50000 [==============================] - 4s 83us/sample - loss: 0.4406 - accuracy: 0.8465 - val_loss: 0.4496 - val_accuracy: 0.8401\nEpoch 5/10\n50000/50000 [==============================] - 5s 99us/sample - loss: 0.4171 - accuracy: 0.8544 - val_loss: 0.4330 - val_accuracy: 0.8439\nEpoch 6/10\n50000/50000 [==============================] - 5s 96us/sample - loss: 0.3985 - accuracy: 0.8609 - val_loss: 0.4198 - val_accuracy: 0.8485\nEpoch 7/10\n50000/50000 [==============================] - 4s 77us/sample - loss: 0.3839 - accuracy: 0.8655 - val_loss: 0.4096 - val_accuracy: 0.8518\nEpoch 8/10\n50000/50000 [==============================] - 4s 87us/sample - loss: 0.3723 - accuracy: 0.8695 - val_loss: 0.4008 - val_accuracy: 0.8559\nEpoch 9/10\n50000/50000 [==============================] - 4s 85us/sample - loss: 0.3610 - accuracy: 0.8731 - val_loss: 0.3943 - val_accuracy: 0.8586\nEpoch 10/10\n50000/50000 [==============================] - 4s 74us/sample - loss: 0.3523 - accuracy: 0.8766 - val_loss: 0.3880 - val_accuracy: 0.8625\n"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300, activation = 'relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')    \n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'], optimizer = 'SGD')\n",
    "history = model.fit(xtrain, ytrain, validation_data = (xval, yval), epochs = 10, batch_size = 256)\n",
    "bn_acc, bn_valacc = history.history['accuracy'], history.history['val_accuracy']\n",
    "bn_loss, bn_valloss = history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_10\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_10 (Flatten)         (None, 784)               0         \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 784)               3136      \n_________________________________________________________________\ndense_30 (Dense)             (None, 300)               235500    \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 300)               1200      \n_________________________________________________________________\ndense_31 (Dense)             (None, 100)               30100     \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 100)               400       \n_________________________________________________________________\ndense_32 (Dense)             (None, 10)                1010      \n=================================================================\nTotal params: 271,346\nTrainable params: 268,978\nNon-trainable params: 2,368\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "source": [
    "* The number of parameters in batch norm are 4*number of features. 4 corresponding to $\\gamma, \\beta$ (the scaling and shifting of normalized inputs), $\\mu, \\sigma$ (moving avg of mean, std dev of training data)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-922fb0752c63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m121\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'RELU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselu_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SELU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Accuracy\")\n",
    "sns.lineplot(np.arange(10), relu_acc, label='RELU')\n",
    "sns.lineplot(np.arange(10), selu_acc, label='SELU')\n",
    "sns.lineplot(np.arange(10), bn_acc, label='Batch_Norm_RELU')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Validation Accuracy\")\n",
    "sns.lineplot(np.arange(10), relu_valacc, label='RELU')\n",
    "sns.lineplot(np.arange(10), selu_valacc, label='SELU')\n",
    "sns.lineplot(np.arange(10), bn_valacc, label='Batch_Norm_RELU')"
   ]
  },
  {
   "source": [
    "### Observations\n",
    "\n",
    "* SELU and BatchNorm are almost similar performance. They indeed do similar transformation\n",
    "\n",
    "* If you use SELU it does take care of normalizations and separate BatchNorm layer is not required"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 50000 samples, validate on 10000 samples\nEpoch 1/10\n50000/50000 [==============================] - 5s 102us/sample - loss: 0.9604 - accuracy: 0.6995 - val_loss: 0.8965 - val_accuracy: 0.7803\nEpoch 2/10\n50000/50000 [==============================] - 4s 73us/sample - loss: 0.6059 - accuracy: 0.8040 - val_loss: 0.5868 - val_accuracy: 0.8120\nEpoch 3/10\n50000/50000 [==============================] - 3s 70us/sample - loss: 0.5275 - accuracy: 0.8253 - val_loss: 0.5095 - val_accuracy: 0.8243\nEpoch 4/10\n50000/50000 [==============================] - 4s 72us/sample - loss: 0.4817 - accuracy: 0.8373 - val_loss: 0.4748 - val_accuracy: 0.8354\nEpoch 5/10\n50000/50000 [==============================] - 4s 73us/sample - loss: 0.4528 - accuracy: 0.8470 - val_loss: 0.4522 - val_accuracy: 0.8414\nEpoch 6/10\n50000/50000 [==============================] - 3s 69us/sample - loss: 0.4292 - accuracy: 0.8530 - val_loss: 0.4347 - val_accuracy: 0.8489\nEpoch 7/10\n50000/50000 [==============================] - 4s 75us/sample - loss: 0.4118 - accuracy: 0.8590 - val_loss: 0.4213 - val_accuracy: 0.8536\nEpoch 8/10\n50000/50000 [==============================] - 4s 71us/sample - loss: 0.3970 - accuracy: 0.8629 - val_loss: 0.4098 - val_accuracy: 0.8569\nEpoch 9/10\n50000/50000 [==============================] - 4s 73us/sample - loss: 0.3842 - accuracy: 0.8657 - val_loss: 0.4006 - val_accuracy: 0.8602\nEpoch 10/10\n50000/50000 [==============================] - 4s 74us/sample - loss: 0.3733 - accuracy: 0.8706 - val_loss: 0.3924 - val_accuracy: 0.8644\n"
    }
   ],
   "source": [
    "### Batch Norm without Bias\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300, use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Dense(100, use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'], optimizer = 'SGD')\n",
    "history = model.fit(xtrain, ytrain, validation_data = (xval, yval), epochs = 10, batch_size = 256)\n",
    "bn1_acc, bn1_valacc = history.history['accuracy'], history.history['val_accuracy']\n",
    "bn1_loss, bn1_valloss = history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-7f34b7bdaaf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m121\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'RELU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselu_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SELU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Accuracy\")\n",
    "sns.lineplot(np.arange(10), relu_acc, label='RELU')\n",
    "sns.lineplot(np.arange(10), selu_acc, label='SELU')\n",
    "sns.lineplot(np.arange(10), bn_acc, label='Batch_Norm_RELU')\n",
    "sns.lineplot(np.arange(10), bn1_acc, label='Batch_Norm1_RELU')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Validation Accuracy\")\n",
    "sns.lineplot(np.arange(10), relu_valacc, label='RELU')\n",
    "sns.lineplot(np.arange(10), selu_valacc, label='SELU')\n",
    "sns.lineplot(np.arange(10), bn_valacc, label='Batch_Norm_RELU')\n",
    "sns.lineplot(np.arange(10), bn1_valacc, label='Batch_Norm1_RELU')"
   ]
  },
  {
   "source": [
    "### Observations\n",
    "\n",
    "* Using BN before or after activations doesnt seem to yield clearer difference in behavior"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Reusing Save Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model to save it before reusing it\n",
    "## \"A\" fashion mnist with 8 target class\n",
    "## \"B\" 200 items of Sandals and Shirts\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(xtrain_A, ytrain_A), (xtrain_B, ytrain_B) = split_dataset(xtrain, ytrain)\n",
    "(xval_A, yval_A), (xval_B, yval_B) = split_dataset(xval, yval)\n",
    "(xtest_A, ytest_A), (xtest_B, ytest_B) = split_dataset(xtest, ytest)\n",
    "xtrain_B = xtrain_B[:200]\n",
    "ytrain_B = ytrain_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 39966 samples, validate on 8034 samples\nEpoch 1/10\n39966/39966 [==============================] - 6s 140us/sample - loss: 0.4247 - accuracy: 0.8645 - val_loss: 0.3098 - val_accuracy: 0.8976\nEpoch 2/10\n39966/39966 [==============================] - 3s 69us/sample - loss: 0.2859 - accuracy: 0.9058 - val_loss: 0.2818 - val_accuracy: 0.9073\nEpoch 3/10\n39966/39966 [==============================] - 3s 74us/sample - loss: 0.2658 - accuracy: 0.9111 - val_loss: 0.2696 - val_accuracy: 0.9106\nEpoch 4/10\n39966/39966 [==============================] - 3s 69us/sample - loss: 0.2517 - accuracy: 0.9159 - val_loss: 0.2585 - val_accuracy: 0.9136\nEpoch 5/10\n39966/39966 [==============================] - 3s 74us/sample - loss: 0.2424 - accuracy: 0.9197 - val_loss: 0.2517 - val_accuracy: 0.9161\nEpoch 6/10\n39966/39966 [==============================] - 3s 71us/sample - loss: 0.2336 - accuracy: 0.9233 - val_loss: 0.2469 - val_accuracy: 0.9181\nEpoch 7/10\n39966/39966 [==============================] - 3s 73us/sample - loss: 0.2267 - accuracy: 0.9235 - val_loss: 0.2420 - val_accuracy: 0.9193\nEpoch 8/10\n39966/39966 [==============================] - 3s 72us/sample - loss: 0.2216 - accuracy: 0.9266 - val_loss: 0.2389 - val_accuracy: 0.9198\nEpoch 9/10\n39966/39966 [==============================] - 3s 71us/sample - loss: 0.2169 - accuracy: 0.9272 - val_loss: 0.2366 - val_accuracy: 0.9190\nEpoch 10/10\n39966/39966 [==============================] - 3s 76us/sample - loss: 0.2117 - accuracy: 0.9289 - val_loss: 0.2321 - val_accuracy: 0.9215\n"
    }
   ],
   "source": [
    "model_A = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    tf.keras.layers.Dense(300, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    tf.keras.layers.Dense(8, activation='softmax'),\n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'], optimizer = 'SGD')\n",
    "history = model.fit(xtrain_A, ytrain_A, validation_data = (xval_A, yval_A), epochs = 10, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 200 samples, validate on 1966 samples\nEpoch 1/10\n200/200 [==============================] - 1s 7ms/sample - loss: 5.4450 - accuracy: 0.0250 - val_loss: 6.1537 - val_accuracy: 0.0097\nEpoch 2/10\n200/200 [==============================] - 0s 400us/sample - loss: 4.6137 - accuracy: 0.0550 - val_loss: 5.1401 - val_accuracy: 0.0295\nEpoch 3/10\n200/200 [==============================] - 0s 375us/sample - loss: 3.7738 - accuracy: 0.0750 - val_loss: 4.2130 - val_accuracy: 0.0885\nEpoch 4/10\n200/200 [==============================] - 0s 375us/sample - loss: 2.9982 - accuracy: 0.1400 - val_loss: 3.5184 - val_accuracy: 0.1801\nEpoch 5/10\n200/200 [==============================] - 0s 387us/sample - loss: 2.3966 - accuracy: 0.2250 - val_loss: 3.0472 - val_accuracy: 0.2706\nEpoch 6/10\n200/200 [==============================] - 0s 462us/sample - loss: 1.9631 - accuracy: 0.3400 - val_loss: 2.7182 - val_accuracy: 0.3469\nEpoch 7/10\n200/200 [==============================] - 0s 375us/sample - loss: 1.6413 - accuracy: 0.4300 - val_loss: 2.4794 - val_accuracy: 0.4135\nEpoch 8/10\n200/200 [==============================] - 0s 362us/sample - loss: 1.3950 - accuracy: 0.5100 - val_loss: 2.2969 - val_accuracy: 0.4669\nEpoch 9/10\n200/200 [==============================] - 0s 425us/sample - loss: 1.2017 - accuracy: 0.5700 - val_loss: 2.1514 - val_accuracy: 0.5086\nEpoch 10/10\n200/200 [==============================] - 0s 412us/sample - loss: 1.0457 - accuracy: 0.6100 - val_loss: 2.0317 - val_accuracy: 0.5448\n"
    }
   ],
   "source": [
    "### Without Transfer Learning\n",
    "model_B = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    tf.keras.layers.Dense(300, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    tf.keras.layers.Dense(1, activation='softmax'),\n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'], optimizer = 'SGD')\n",
    "history = model.fit(xtrain_B, ytrain_B, validation_data = (xval_B, yval_B), epochs = 10, batch_size = 256)\n",
    "modelB_scratch_acc, modelB_scratch_valacc = history.history['accuracy'], history.history['val_accuracy']\n",
    "modelB_scratch_loss, modelB_scratch_valloss = history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.keras' has no attribute 'load_model'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-453e046dcbfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_B_with_A\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"my_model_A.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core.keras' has no attribute 'load_model'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "model_B_with_A = keras.load_model(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'2.0.0'"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'2.2.4-tf'"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "keras.__version__"
   ]
  }
 ]
}