{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "display_name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "metadata": {
    "interpreter": {
     "hash": "22b3a9e511cc71c7e8c0c199ed3ba2f3f697f01009167188cfc9ddfa1a3c5b27"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1.  Custom Loss Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Points to note before creating custom loss:\n",
    "\n",
    "* Need to store the thresholds while saving & using the h5 models again with custom loss\n",
    "\n",
    "* In such cases use subclass method compared to simpler way of pythonic functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain, ytrain), (xtest, ytest) = tf.keras.datasets.boston_housing.load_data()\n",
    "scaler = StandardScaler()\n",
    "xtrain = scaler.fit_transform(xtrain)\n",
    "xtest = scaler.transform(xtest)\n",
    "xtrain, xval = xtrain[:350], xtrain[350:]\n",
    "ytrain, yval = ytrain[:350], ytrain[350:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [13]),\n",
    "    tf.keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "model.compile(loss = 'mae', metrics = ['mae'], optimizer = 'Adam')\n",
    "history = model.fit(xtrain, ytrain, epochs = 100, validation_data = (xval, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(100), history.history[\"val_mae\"], label = 'val_error')\n",
    "plt.plot(np.arange(100), history.history[\"mae\"], label = 'training_error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold = 1.0, **kwargs):\n",
    "        ''' takes into account of standard hyperparms, std aggregation of loss (sum_over_batch_size)'''\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        ''' compute loss and returns '''\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_error = tf.square(error)/2\n",
    "        linear_error = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_error, linear_error)\n",
    "\n",
    "    def get_config(self):\n",
    "        ''' returns hyperparams name to its value '''\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'threshold' : self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss = HuberLoss(2.), optimizer = 'Adam')\n",
    "history = model.fit(xtrain, ytrain, epochs = 100, validation_data = (xval, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(100), history.history[\"val_loss\"], label = 'val_error')\n",
    "plt.plot(np.arange(100), history.history[\"loss\"], label = 'training_error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving Model\n",
    "tf.keras.models.save_model(model, \"my_model_custom_loss_class.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'HuberLoss' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d5fa85b96ce6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# model.compile(loss = HuberLoss(2.), optimizer = 'Adam')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_from_saved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"my_model_custom_loss_class.h5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'HuberLoss'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mHuberLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'HuberLoss' is not defined"
     ]
    }
   ],
   "source": [
    "# model.compile(loss = HuberLoss(2.), optimizer = 'Adam')\n",
    "model_from_saved = tf.keras.models.load_model(\"my_model_custom_loss_class.h5\", custom_objects={'HuberLoss' : HuberLoss()}, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_from_saved.compile(loss = HuberLoss(2.), optimizer = 'Adam')\n",
    "\n",
    "model_from_saved.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='3_layer_mlp')\n",
    "\n",
    "# Useless custom loss here\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return keras.backend.mean(keras.backend.square(y_true - y_pred), axis=-1)\n",
    "\n",
    "model.save(\"model\", save_format='tf')\n",
    "model.compile(loss=custom_loss, optimizer=keras.optimizers.RMSprop())\n",
    "# Here comes the bug (no bug)\n",
    "new_model = keras.models.load_model('model', custom_objects={'loss': custom_loss})"
   ]
  },
  {
   "source": [
    "### 2. Custom Activations, Regularizers, Initializers\n",
    "\n",
    "#### Points to note:\n",
    "* Use always tf functionality to maximise the performance\n",
    "\n",
    "* you must implement the call() method for losses, layers (including activation\n",
    "functions), and models, or the __call__() method for regularizers, initializers,\n",
    "and constraints."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(z):\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def glorot_initializer(shape, dtype = tf.float32):\n",
    "    stddev = tf.sqrt(2./(shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, mean=0, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.1 * weights))\n",
    "\n",
    "def relu_constraint(weights):\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain, ytrain), (xtest, ytest) = tf.keras.datasets.boston_housing.load_data()\n",
    "scaler = StandardScaler()\n",
    "xtrain = scaler.fit_transform(xtrain)\n",
    "xtest = scaler.transform(xtest)\n",
    "xtrain, xval = xtrain[:350], xtrain[350:]\n",
    "ytrain, yval = ytrain[:350], ytrain[350:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 350 samples, validate on 54 samples\nEpoch 1/100\n350/350 [==============================] - 1s 2ms/sample - loss: 24.9289 - mae: 23.0378 - val_loss: 24.8066 - val_mae: 23.0778\nEpoch 2/100\n350/350 [==============================] - 0s 126us/sample - loss: 24.6206 - mae: 22.9163 - val_loss: 24.6217 - val_mae: 22.9471\nEpoch 3/100\n350/350 [==============================] - 0s 140us/sample - loss: 24.4236 - mae: 22.7728 - val_loss: 24.4392 - val_mae: 22.8169\nEpoch 4/100\n350/350 [==============================] - 0s 134us/sample - loss: 24.2302 - mae: 22.6307 - val_loss: 24.2590 - val_mae: 22.6872\nEpoch 5/100\n350/350 [==============================] - 0s 137us/sample - loss: 24.0379 - mae: 22.4883 - val_loss: 24.0825 - val_mae: 22.5594\nEpoch 6/100\n350/350 [==============================] - 0s 146us/sample - loss: 23.8508 - mae: 22.3484 - val_loss: 23.9094 - val_mae: 22.4316\nEpoch 7/100\n350/350 [==============================] - 0s 143us/sample - loss: 23.6683 - mae: 22.2099 - val_loss: 23.7364 - val_mae: 22.3017\nEpoch 8/100\n350/350 [==============================] - 0s 163us/sample - loss: 23.4833 - mae: 22.0679 - val_loss: 23.5664 - val_mae: 22.1745\nEpoch 9/100\n350/350 [==============================] - 0s 146us/sample - loss: 23.3018 - mae: 21.9288 - val_loss: 23.3992 - val_mae: 22.0486\nEpoch 10/100\n350/350 [==============================] - 0s 166us/sample - loss: 23.1253 - mae: 21.7922 - val_loss: 23.2326 - val_mae: 21.9206\nEpoch 11/100\n350/350 [==============================] - 0s 163us/sample - loss: 22.9482 - mae: 21.6539 - val_loss: 23.0686 - val_mae: 21.7948\nEpoch 12/100\n350/350 [==============================] - 0s 180us/sample - loss: 22.7757 - mae: 21.5179 - val_loss: 22.9032 - val_mae: 21.6649\nEpoch 13/100\n350/350 [==============================] - 0s 186us/sample - loss: 22.6019 - mae: 21.3788 - val_loss: 22.7395 - val_mae: 21.5351\nEpoch 14/100\n350/350 [==============================] - 0s 146us/sample - loss: 22.4281 - mae: 21.2386 - val_loss: 22.5769 - val_mae: 21.4060\nEpoch 15/100\n350/350 [==============================] - 0s 140us/sample - loss: 22.2561 - mae: 21.0994 - val_loss: 22.4159 - val_mae: 21.2755\nEpoch 16/100\n350/350 [==============================] - 0s 131us/sample - loss: 22.0860 - mae: 20.9585 - val_loss: 22.2580 - val_mae: 21.1455\nEpoch 17/100\n350/350 [==============================] - 0s 123us/sample - loss: 21.9192 - mae: 20.8183 - val_loss: 22.1006 - val_mae: 21.0134\nEpoch 18/100\n350/350 [==============================] - 0s 143us/sample - loss: 21.7543 - mae: 20.6771 - val_loss: 21.9444 - val_mae: 20.8799\nEpoch 19/100\n350/350 [==============================] - 0s 140us/sample - loss: 21.5880 - mae: 20.5339 - val_loss: 21.7903 - val_mae: 20.7486\nEpoch 20/100\n350/350 [==============================] - 0s 146us/sample - loss: 21.4259 - mae: 20.3940 - val_loss: 21.6371 - val_mae: 20.6167\nEpoch 21/100\n350/350 [==============================] - 0s 171us/sample - loss: 21.2630 - mae: 20.2522 - val_loss: 21.4838 - val_mae: 20.4842\nEpoch 22/100\n350/350 [==============================] - 0s 123us/sample - loss: 21.1039 - mae: 20.1124 - val_loss: 21.3288 - val_mae: 20.3475\nEpoch 23/100\n350/350 [==============================] - 0s 120us/sample - loss: 20.9408 - mae: 19.9679 - val_loss: 21.1779 - val_mae: 20.2145\nEpoch 24/100\n350/350 [==============================] - 0s 47us/sample - loss: 20.7838 - mae: 19.8275 - val_loss: 21.0273 - val_mae: 20.0786\nEpoch 25/100\n350/350 [==============================] - 0s 123us/sample - loss: 20.6279 - mae: 19.6842 - val_loss: 20.8761 - val_mae: 19.9389\nEpoch 26/100\n350/350 [==============================] - 0s 131us/sample - loss: 20.4679 - mae: 19.5361 - val_loss: 20.7248 - val_mae: 19.7992\nEpoch 27/100\n350/350 [==============================] - 0s 143us/sample - loss: 20.3089 - mae: 19.3879 - val_loss: 20.5722 - val_mae: 19.6566\nEpoch 28/100\n350/350 [==============================] - 0s 131us/sample - loss: 20.1487 - mae: 19.2374 - val_loss: 20.4256 - val_mae: 19.5194\nEpoch 29/100\n350/350 [==============================] - 0s 200us/sample - loss: 19.9876 - mae: 19.0850 - val_loss: 20.2812 - val_mae: 19.3834\nEpoch 30/100\n350/350 [==============================] - 0s 194us/sample - loss: 19.8253 - mae: 18.9313 - val_loss: 20.1372 - val_mae: 19.2478\nEpoch 31/100\n350/350 [==============================] - 0s 211us/sample - loss: 19.6655 - mae: 18.7801 - val_loss: 19.9925 - val_mae: 19.1115\nEpoch 32/100\n350/350 [==============================] - 0s 146us/sample - loss: 19.5063 - mae: 18.6283 - val_loss: 19.8463 - val_mae: 18.9721\nEpoch 33/100\n350/350 [==============================] - 0s 326us/sample - loss: 19.3457 - mae: 18.4742 - val_loss: 19.6992 - val_mae: 18.8310\nEpoch 34/100\n350/350 [==============================] - 0s 257us/sample - loss: 19.1873 - mae: 18.3220 - val_loss: 19.5527 - val_mae: 18.6905\nEpoch 35/100\n350/350 [==============================] - 0s 297us/sample - loss: 19.0311 - mae: 18.1713 - val_loss: 19.4079 - val_mae: 18.5510\nEpoch 36/100\n350/350 [==============================] - 0s 166us/sample - loss: 18.8808 - mae: 18.0262 - val_loss: 19.2612 - val_mae: 18.4093\nEpoch 37/100\n350/350 [==============================] - 0s 177us/sample - loss: 18.7312 - mae: 17.8817 - val_loss: 19.1143 - val_mae: 18.2680\nEpoch 38/100\n350/350 [==============================] - 0s 143us/sample - loss: 18.5826 - mae: 17.7385 - val_loss: 18.9648 - val_mae: 18.1235\nEpoch 39/100\n350/350 [==============================] - 0s 191us/sample - loss: 18.4306 - mae: 17.5920 - val_loss: 18.8166 - val_mae: 17.9809\nEpoch 40/100\n350/350 [==============================] - 0s 174us/sample - loss: 18.2829 - mae: 17.4494 - val_loss: 18.6661 - val_mae: 17.8352\nEpoch 41/100\n350/350 [==============================] - 0s 183us/sample - loss: 18.1315 - mae: 17.3034 - val_loss: 18.5158 - val_mae: 17.6910\nEpoch 42/100\n350/350 [==============================] - 0s 154us/sample - loss: 17.9837 - mae: 17.1611 - val_loss: 18.3637 - val_mae: 17.5436\nEpoch 43/100\n350/350 [==============================] - 0s 171us/sample - loss: 17.8357 - mae: 17.0173 - val_loss: 18.2101 - val_mae: 17.3935\nEpoch 44/100\n350/350 [==============================] - 0s 183us/sample - loss: 17.6846 - mae: 16.8695 - val_loss: 18.0542 - val_mae: 17.2410\nEpoch 45/100\n350/350 [==============================] - 0s 163us/sample - loss: 17.5314 - mae: 16.7201 - val_loss: 17.8979 - val_mae: 17.0886\nEpoch 46/100\n350/350 [==============================] - 0s 166us/sample - loss: 17.3802 - mae: 16.5724 - val_loss: 17.7362 - val_mae: 16.9300\nEpoch 47/100\n350/350 [==============================] - 0s 449us/sample - loss: 17.2245 - mae: 16.4200 - val_loss: 17.5753 - val_mae: 16.7732\nEpoch 48/100\n350/350 [==============================] - 0s 257us/sample - loss: 17.0708 - mae: 16.2707 - val_loss: 17.4179 - val_mae: 16.6204\nEpoch 49/100\n350/350 [==============================] - 0s 226us/sample - loss: 16.9188 - mae: 16.1232 - val_loss: 17.2623 - val_mae: 16.4691\nEpoch 50/100\n350/350 [==============================] - 0s 191us/sample - loss: 16.7647 - mae: 15.9733 - val_loss: 17.1105 - val_mae: 16.3210\nEpoch 51/100\n350/350 [==============================] - 0s 154us/sample - loss: 16.6102 - mae: 15.8217 - val_loss: 16.9574 - val_mae: 16.1703\nEpoch 52/100\n350/350 [==============================] - 0s 211us/sample - loss: 16.4522 - mae: 15.6659 - val_loss: 16.8029 - val_mae: 16.0176\nEpoch 53/100\n350/350 [==============================] - 0s 151us/sample - loss: 16.2920 - mae: 15.5075 - val_loss: 16.6457 - val_mae: 15.8623\nEpoch 54/100\n350/350 [==============================] - 0s 183us/sample - loss: 16.1318 - mae: 15.3493 - val_loss: 16.4852 - val_mae: 15.7040\nEpoch 55/100\n350/350 [==============================] - 0s 160us/sample - loss: 15.9692 - mae: 15.1897 - val_loss: 16.3263 - val_mae: 15.5487\nEpoch 56/100\n350/350 [==============================] - 0s 234us/sample - loss: 15.8070 - mae: 15.0309 - val_loss: 16.1700 - val_mae: 15.3952\nEpoch 57/100\n350/350 [==============================] - 0s 154us/sample - loss: 15.6470 - mae: 14.8736 - val_loss: 16.0083 - val_mae: 15.2367\nEpoch 58/100\n350/350 [==============================] - 0s 166us/sample - loss: 15.4828 - mae: 14.7127 - val_loss: 15.8461 - val_mae: 15.0777\nEpoch 59/100\n350/350 [==============================] - 0s 154us/sample - loss: 15.3191 - mae: 14.5525 - val_loss: 15.6823 - val_mae: 14.9176\nEpoch 60/100\n350/350 [==============================] - 0s 186us/sample - loss: 15.1546 - mae: 14.3917 - val_loss: 15.5207 - val_mae: 14.7596\nEpoch 61/100\n350/350 [==============================] - 0s 134us/sample - loss: 14.9947 - mae: 14.2345 - val_loss: 15.3588 - val_mae: 14.5994\nEpoch 62/100\n350/350 [==============================] - 0s 154us/sample - loss: 14.8353 - mae: 14.0764 - val_loss: 15.1991 - val_mae: 14.4408\nEpoch 63/100\n350/350 [==============================] - 0s 129us/sample - loss: 14.6800 - mae: 13.9216 - val_loss: 15.0351 - val_mae: 14.2767\nEpoch 64/100\n350/350 [==============================] - 0s 140us/sample - loss: 14.5162 - mae: 13.7587 - val_loss: 14.8742 - val_mae: 14.1167\nEpoch 65/100\n350/350 [==============================] - 0s 166us/sample - loss: 14.3604 - mae: 13.6023 - val_loss: 14.7131 - val_mae: 13.9541\nEpoch 66/100\n350/350 [==============================] - 0s 166us/sample - loss: 14.2012 - mae: 13.4419 - val_loss: 14.5521 - val_mae: 13.7914\nEpoch 67/100\n350/350 [==============================] - 0s 183us/sample - loss: 14.0442 - mae: 13.2817 - val_loss: 14.3856 - val_mae: 13.6211\nEpoch 68/100\n350/350 [==============================] - 0s 197us/sample - loss: 13.8787 - mae: 13.1130 - val_loss: 14.2206 - val_mae: 13.4539\nEpoch 69/100\n350/350 [==============================] - 0s 166us/sample - loss: 13.7148 - mae: 12.9466 - val_loss: 14.0506 - val_mae: 13.2809\nEpoch 70/100\n350/350 [==============================] - 0s 154us/sample - loss: 13.5446 - mae: 12.7731 - val_loss: 13.8833 - val_mae: 13.1097\nEpoch 71/100\n350/350 [==============================] - 0s 154us/sample - loss: 13.3727 - mae: 12.5973 - val_loss: 13.7160 - val_mae: 12.9386\nEpoch 72/100\n350/350 [==============================] - 0s 217us/sample - loss: 13.2056 - mae: 12.4265 - val_loss: 13.5441 - val_mae: 12.7627\nEpoch 73/100\n350/350 [==============================] - 0s 160us/sample - loss: 13.0338 - mae: 12.2495 - val_loss: 13.3695 - val_mae: 12.5826\nEpoch 74/100\n350/350 [==============================] - 0s 183us/sample - loss: 12.8601 - mae: 12.0711 - val_loss: 13.1925 - val_mae: 12.4007\nEpoch 75/100\n350/350 [==============================] - 0s 146us/sample - loss: 12.6820 - mae: 11.8872 - val_loss: 13.0190 - val_mae: 12.2210\nEpoch 76/100\n350/350 [==============================] - 0s 471us/sample - loss: 12.5101 - mae: 11.7093 - val_loss: 12.8389 - val_mae: 12.0345\nEpoch 77/100\n350/350 [==============================] - 0s 243us/sample - loss: 12.3277 - mae: 11.5207 - val_loss: 12.6631 - val_mae: 11.8521\nEpoch 78/100\n350/350 [==============================] - 0s 206us/sample - loss: 12.1550 - mae: 11.3385 - val_loss: 12.4774 - val_mae: 11.6556\nEpoch 79/100\n350/350 [==============================] - 0s 143us/sample - loss: 11.9633 - mae: 11.1384 - val_loss: 12.2934 - val_mae: 11.4644\nEpoch 80/100\n350/350 [==============================] - 0s 186us/sample - loss: 11.7825 - mae: 10.9484 - val_loss: 12.1045 - val_mae: 11.2645\nEpoch 81/100\n350/350 [==============================] - 0s 220us/sample - loss: 11.5940 - mae: 10.7484 - val_loss: 11.9129 - val_mae: 11.0614\nEpoch 82/100\n350/350 [==============================] - 0s 200us/sample - loss: 11.4005 - mae: 10.5437 - val_loss: 11.7187 - val_mae: 10.8563\nEpoch 83/100\n350/350 [==============================] - 0s 180us/sample - loss: 11.2039 - mae: 10.3365 - val_loss: 11.5188 - val_mae: 10.6454\nEpoch 84/100\n350/350 [==============================] - 0s 154us/sample - loss: 11.0034 - mae: 10.1251 - val_loss: 11.3170 - val_mae: 10.4332\nEpoch 85/100\n350/350 [==============================] - 0s 463us/sample - loss: 10.8029 - mae: 9.9139 - val_loss: 11.1126 - val_mae: 10.2170\nEpoch 86/100\n350/350 [==============================] - 0s 229us/sample - loss: 10.5999 - mae: 9.6974 - val_loss: 10.9067 - val_mae: 9.9969\nEpoch 87/100\n350/350 [==============================] - 0s 246us/sample - loss: 10.3928 - mae: 9.4770 - val_loss: 10.7100 - val_mae: 9.7870\nEpoch 88/100\n350/350 [==============================] - 0s 223us/sample - loss: 10.1912 - mae: 9.2619 - val_loss: 10.5112 - val_mae: 9.5753\nEpoch 89/100\n350/350 [==============================] - 0s 189us/sample - loss: 9.9853 - mae: 9.0436 - val_loss: 10.3142 - val_mae: 9.3660\nEpoch 90/100\n350/350 [==============================] - 0s 143us/sample - loss: 9.7808 - mae: 8.8283 - val_loss: 10.1202 - val_mae: 9.1623\nEpoch 91/100\n350/350 [==============================] - 0s 169us/sample - loss: 9.5878 - mae: 8.6266 - val_loss: 9.9247 - val_mae: 8.9588\nEpoch 92/100\n350/350 [==============================] - 0s 189us/sample - loss: 9.3963 - mae: 8.4267 - val_loss: 9.7307 - val_mae: 8.7571\nEpoch 93/100\n350/350 [==============================] - 0s 157us/sample - loss: 9.2097 - mae: 8.2327 - val_loss: 9.5343 - val_mae: 8.5531\nEpoch 94/100\n350/350 [==============================] - 0s 151us/sample - loss: 9.0225 - mae: 8.0389 - val_loss: 9.3452 - val_mae: 8.3587\nEpoch 95/100\n350/350 [==============================] - 0s 180us/sample - loss: 8.8411 - mae: 7.8522 - val_loss: 9.1559 - val_mae: 8.1641\nEpoch 96/100\n350/350 [==============================] - 0s 369us/sample - loss: 8.6668 - mae: 7.6721 - val_loss: 8.9644 - val_mae: 7.9668\nEpoch 97/100\n350/350 [==============================] - 0s 186us/sample - loss: 8.4962 - mae: 7.4973 - val_loss: 8.7786 - val_mae: 7.7786\nEpoch 98/100\n350/350 [==============================] - 0s 186us/sample - loss: 8.3324 - mae: 7.3311 - val_loss: 8.5917 - val_mae: 7.5889\nEpoch 99/100\n350/350 [==============================] - 0s 166us/sample - loss: 8.1652 - mae: 7.1624 - val_loss: 8.4149 - val_mae: 7.4107\nEpoch 100/100\n350/350 [==============================] - 0s 209us/sample - loss: 8.0171 - mae: 7.0116 - val_loss: 8.2404 - val_mae: 7.2331\n"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [13]),\n",
    "    tf.keras.layers.Dense(10, activation=softplus, kernel_initializer=glorot_initializer,\n",
    "                         kernel_regularizer=l1_regularizer, kernel_constraint=relu_constraint),\n",
    "    tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "model.compile(loss = 'mae', metrics = ['mae'], optimizer = 'Adam')\n",
    "history = model.fit(xtrain, ytrain, epochs = 100, validation_data = (xval, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use subclass approach if you want to save l1 regularizer factor along with model\n",
    "class l1_regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor, **kwargs):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'factor' : self.factor}"
   ]
  },
  {
   "source": [
    "### 3. Custom Layers\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this layer creates exponential of inputs\n",
    "exponential_layer = tf.keras.layers.Lambda(lambda x : tf.exp(x))"
   ]
  }
 ]
}