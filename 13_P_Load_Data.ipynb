{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "display_name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "metadata": {
    "interpreter": {
     "hash": "22b3a9e511cc71c7e8c0c199ed3ba2f3f697f01009167188cfc9ddfa1a3c5b27"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "source": [
    "## From Tensor Slices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.Variable(tf.range(10))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x : x**2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x : x % 2 == 0)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "source": [
    "## Categorical Data\n",
    "\n",
    "### One-hot Vs Embedding\n",
    "\n",
    "As a rule of thumb, if the number of categories is lower than 10,\n",
    "then one-hot encoding is generally the way to go (but your mileage\n",
    "may vary!). If the number of categories is greater than 50 (which is\n",
    "often the case when you use hash buckets), then embeddings are\n",
    "usually preferable. In between 10 and 50 categories, you may want\n",
    "to experiment with both options and see which one works best for\n",
    "your use case.\n",
    "\n",
    "### One-hot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "fetch_housing_data()\n",
    "import pandas as pd\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_housing_data()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing table\n",
    "ocean_list = list(df['ocean_proximity'].unique())\n",
    "indices = tf.range(len(ocean_list), dtype = tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(ocean_list, indices)\n",
    "num_oov = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## one-hot\n",
    "random_categories = tf.constant([random.sample(ocean_list, 1)[0] for i in range(4)])\n",
    "print(random_categories)\n",
    "cat_indicies = table.lookup(random_categories)\n",
    "cat_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.one_hot(cat_indicies, depth = len(ocean_list) + num_oov)"
   ]
  },
  {
   "source": [
    "### Embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "embed_init = tf.random.uniform(shape = [(len(ocean_list) + num_oov), embedding_dim])\n",
    "embed_matrix = tf.Variable(embed_init)\n",
    "embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(random_categories)\n",
    "tf.nn.embedding_lookup(embed_matrix, cat_indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keras layer : random_init (but witholds the cat_ind)\n",
    "embed_layer = tf.keras.layers.Embedding(input_dim=len(ocean_list)+num_oov, output_dim=embedding_dim)\n",
    "embed_layer(cat_indicies)"
   ]
  },
  {
   "source": [
    "One-hot encoding followed by a Dense layer (with no activation\n",
    "function and no biases) is equivalent to an Embedding layer. However,\n",
    "the Embedding layer uses way fewer computations (the performance\n",
    "difference becomes clear when the size of the embedding\n",
    "matrix grows). The Dense layerâ€™s weight matrix plays the role of the\n",
    "embedding matrix. For example, using one-hot vectors of size 20\n",
    "and a Dense layer with 10 units is equivalent to using an Embedding\n",
    "layer with input_dim=20 and output_dim=10. As a result, it would\n",
    "be wasteful to use more embedding dimensions than the number\n",
    "of units in the layer that follows the Embedding layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = tfds.load(name = \"mnist\")\n",
    "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.shuffle(1000).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items : (items[\"image\"], items[\"label\"]))\n",
    "mnist_train = mnist_train.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 32 steps\nEpoch 1/5\n32/32 [==============================] - 1s 19ms/step - loss: 54.0883 - accuracy: 0.2764\nEpoch 2/5\n32/32 [==============================] - 0s 3ms/step - loss: 5.4162 - accuracy: 0.3506\nEpoch 3/5\n32/32 [==============================] - 0s 3ms/step - loss: 2.7378 - accuracy: 0.2705\nEpoch 4/5\n32/32 [==============================] - 0s 3ms/step - loss: 2.4293 - accuracy: 0.2852\nEpoch 5/5\n32/32 [==============================] - 0s 3ms/step - loss: 2.2371 - accuracy: 0.3252\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x13071648>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [28, 28, 1]),\n",
    "    tf.keras.layers.Dense(30, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
    "model.fit(mnist_train, steps_per_epoch = 32, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}